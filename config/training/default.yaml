# Training configuration

# Optimizer
optimizer:
  type: adam
  lr: 0.001
  weight_decay: 0.0001

# Learning rate scheduler
scheduler:
  type: reduce_on_plateau
  mode: min
  factor: 0.5
  patience: 10
  min_lr: 0.00001

# Training loop
epochs: 100
batch_size: 32
accumulation_steps: 1  # Gradient accumulation

# Loss function
loss:
  # MSE on valid laps only (masked)
  type: mse

  # Regularization
  regularization:
    embedding_norm_penalty: 0.001
    driver_shrinkage: 0.01  # Shrink driver contributions
    tyre_smoothness_penalty: 0.0  # Optional: penalize non-smooth degradation

# Validation
validation:
  split_ratio: 0.15  # Use 15% of stints for validation
  eval_every_n_epochs: 1

# Checkpointing
checkpoint:
  save_every_n_epochs: 5
  save_best_only: true
  monitor_metric: val_rmse

# Logging
logging:
  use_tensorboard: true
  log_dir: ${paths.logs_dir}
  log_every_n_batches: 50

# Early stopping
early_stopping:
  enabled: true
  patience: 20
  monitor_metric: val_rmse
  min_delta: 0.001

# Hardware
device: auto  # auto, cpu, cuda, mps
num_workers: 4  # DataLoader workers
